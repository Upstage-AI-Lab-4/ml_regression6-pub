{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 10:29:22.195846: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-13 10:29:22.198334: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-13 10:29:22.207371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-13 10:29:22.222274: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-13 10:29:22.226799: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-13 10:29:22.237266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-13 10:29:22.915976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['번지', '본번', '부번', '아파트명', '전용면적', '계약일', '층', '건축년도', 'target',\n",
       "       'is_test', '주소', 'x', 'y', '계약년', '계약월', '거래취소여부', '거래일건물연식', '구', '동',\n",
       "       '강남여부', '신축여부', '건축면적', '연면적', '대지면적', '건폐율', '용적율', '평균층수'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "concat = pd.read_csv('../data/base_dataset_7.csv')\n",
    "concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 변수: ['전용면적', '계약일', '층', '건축년도', 'target', 'is_test', 'x', 'y', '계약년', '계약월', '거래일건물연식', '건축면적', '연면적', '대지면적', '건폐율', '용적율', '평균층수']\n",
      "범주형 변수: ['번지', '본번', '부번', '아파트명', '주소', '거래취소여부', '구', '동', '강남여부', '신축여부']\n"
     ]
    }
   ],
   "source": [
    "str_columns = ['본번', '부번', '거래취소여부', '구', '동', '강남여부', '신축여부']\n",
    "\n",
    "concat[str_columns] = concat[str_columns].astype(str)\n",
    "\n",
    "# 먼저, 연속형 변수와 범주형 변수를 위 info에 따라 분리해주겠습니다.\n",
    "continuous_columns = []\n",
    "categorical_columns = []\n",
    "\n",
    "for column in concat.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat[column]):\n",
    "        continuous_columns.append(column)\n",
    "    else:\n",
    "        categorical_columns.append(column)\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns)\n",
    "print(\"범주형 변수:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번지         0\n",
      "본번         0\n",
      "부번         0\n",
      "아파트명       0\n",
      "전용면적       0\n",
      "계약일        0\n",
      "층          0\n",
      "건축년도       0\n",
      "target     0\n",
      "is_test    0\n",
      "주소         0\n",
      "x          0\n",
      "y          0\n",
      "계약년        0\n",
      "계약월        0\n",
      "거래취소여부     0\n",
      "거래일건물연식    0\n",
      "구          0\n",
      "동          0\n",
      "강남여부       0\n",
      "신축여부       0\n",
      "건축면적       0\n",
      "연면적        0\n",
      "대지면적       0\n",
      "건폐율        0\n",
      "용적율        0\n",
      "평균층수       0\n",
      "dtype: int64\n",
      "(1128094, 27)\n"
     ]
    }
   ],
   "source": [
    "# 범주형 변수에 대한 보간\n",
    "concat[categorical_columns] = concat[categorical_columns].fillna('NULL')\n",
    "\n",
    "# 연속형 변수에 대한 보간 (선형 보간)\n",
    "concat[continuous_columns] = concat[continuous_columns].interpolate(method='linear', axis=0)\n",
    "\n",
    "print(concat.isnull().sum())         # 결측치가 보간된 모습을 확인해봅니다.\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1108303 entries, 0 to 1128093\n",
      "Data columns (total 27 columns):\n",
      " #   Column   Non-Null Count    Dtype  \n",
      "---  ------   --------------    -----  \n",
      " 0   번지       1108303 non-null  object \n",
      " 1   본번       1108303 non-null  object \n",
      " 2   부번       1108303 non-null  object \n",
      " 3   아파트명     1108303 non-null  object \n",
      " 4   전용면적     1108303 non-null  float64\n",
      " 5   계약일      1108303 non-null  int64  \n",
      " 6   층        1108303 non-null  int64  \n",
      " 7   건축년도     1108303 non-null  int64  \n",
      " 8   target   1108303 non-null  float64\n",
      " 9   is_test  1108303 non-null  int64  \n",
      " 10  주소       1108303 non-null  object \n",
      " 11  x        1108303 non-null  float64\n",
      " 12  y        1108303 non-null  float64\n",
      " 13  계약년      1108303 non-null  int64  \n",
      " 14  계약월      1108303 non-null  int64  \n",
      " 15  거래취소여부   1108303 non-null  object \n",
      " 16  거래일건물연식  1108303 non-null  int64  \n",
      " 17  구        1108303 non-null  object \n",
      " 18  동        1108303 non-null  object \n",
      " 19  강남여부     1108303 non-null  object \n",
      " 20  신축여부     1108303 non-null  object \n",
      " 21  건축면적     1108303 non-null  float64\n",
      " 22  연면적      1108303 non-null  float64\n",
      " 23  대지면적     1108303 non-null  float64\n",
      " 24  건폐율      1108303 non-null  float64\n",
      " 25  용적율      1108303 non-null  float64\n",
      " 26  평균층수     1108303 non-null  float64\n",
      "dtypes: float64(10), int64(7), object(10)\n",
      "memory usage: 236.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# test와 train 분리\n",
    "df = concat.query('is_test == 0')  \n",
    "df_test = concat.query('is_test == 1')  \n",
    "\n",
    "# 전용면적 375 이상인 것들은 제거\n",
    "df= df[df['전용면적'] < 375]\n",
    "\n",
    "# 층 음수 처리\n",
    "df['층'] = df['층'].apply(lambda x: 1 if x < 0 else x)\n",
    "df_test['층'] = df_test['층'].apply(lambda x: 1 if x < 0 else x)\n",
    "\n",
    "# 건축년도 1965년도 이후에 지어진것만 처리\n",
    "df= df[df['건축년도'] > 1965]\n",
    "\n",
    "# 거래일 건물 연식 음수 처리\n",
    "df= df[df['거래일건물연식'] > 0]\n",
    "concat_select = pd.concat([df, df_test])\n",
    "concat_select.info()       # 최종 데이터셋은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1108303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1108303/1108303 [00:23<00:00, 46211.84it/s]\n",
      "100%|██████████| 1108303/1108303 [04:06<00:00, 4491.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.108303e+06\n",
       "mean     1.999190e+00\n",
       "std      3.362743e-02\n",
       "min      0.000000e+00\n",
       "25%      2.000000e+00\n",
       "50%      2.000000e+00\n",
       "75%      2.000000e+00\n",
       "max      2.000000e+00\n",
       "Name: 버스최단거리, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 하버사인 공식으로 두 지리적 좌표 간의 거리를 계산하는 함수\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    R = 6371  # 지구 반경 (킬로미터)\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# 최단 거리 계산 함수\n",
    "def calculate_shortest_distances(concat_coords, facility_coords):\n",
    "    shortest_distances = []\n",
    "    for concat_coord in tqdm(concat_coords, total=len(concat_coords)):\n",
    "        distances = haversine_np(concat_coord[0], concat_coord[1], facility_coords[:, 0], facility_coords[:, 1])\n",
    "        min_distance = np.min(distances)\n",
    "        shortest_distances.append(min_distance)\n",
    "    return shortest_distances\n",
    "\n",
    "# 데이터 로드\n",
    "subway_path = pd.read_csv('/Users/minseok/Downloads/xgboost/data/subway_feature.csv')\n",
    "bus_path = pd.read_csv('/Users/minseok/Downloads/xgboost/data/bus_feature.csv')\n",
    "\n",
    "# 좌표 배열로 변환\n",
    "concat_coords = np.array([concat_select['x'], concat_select['y']]).T\n",
    "subway_coords = np.array([subway_path['경도'], subway_path['위도']]).T\n",
    "bus_coords = np.array([bus_path['X좌표'], bus_path['Y좌표']]).T\n",
    "\n",
    "# 지하철과 버스 최단 거리 계산\n",
    "concat_select['지하철최단거리'] = calculate_shortest_distances(concat_coords, subway_coords)\n",
    "concat_select['버스최단거리'] = calculate_shortest_distances(concat_coords, bus_coords)\n",
    "\n",
    "# 거리별 가중치 부여 함수\n",
    "def assign_weights(distance, thresholds, weights):\n",
    "    for t, w in zip(thresholds, weights):\n",
    "        if distance < t:\n",
    "            return w\n",
    "    return weights[-1]\n",
    "\n",
    "# 거리별 가중치 부여\n",
    "concat_select['지하철최단거리'] = concat_select['지하철최단거리'].apply(lambda x: assign_weights(x, [0.3, 0.65, 1.0], [3, 2, 1, 0]))\n",
    "concat_select['버스최단거리'] = concat_select['버스최단거리'].apply(lambda x: assign_weights(x, [0.5, 1.0], [2, 1, 0]))\n",
    "\n",
    "# 결과 확인\n",
    "concat_select['버스최단거리'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1108303, 29) Index(['번지', '본번', '부번', '전용면적', '계약일', '층', '건축년도', 'target', 'is_test', '주소',\n",
      "       'x', 'y', '계약년', '계약월', '거래취소여부', '거래일건물연식', '구', '동', '강남여부', '신축여부',\n",
      "       '건축면적', '연면적', '대지면적', '건폐율', '용적율', '평균층수', '지하철최단거리', '버스최단거리',\n",
      "       '아파트브랜드이름'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "brand_patterns = {\n",
    "   #1군 하이엔트\n",
    "   '디에이치':3, '푸르지오써밋': 3, '르엘':3,'오티에르':3, '아크로':3,\n",
    "   #1군\n",
    "   '힐스테이트': 2, '푸르지오':2, '롯데캐슬':2, '더샵':2, 'e-편한세상':2,'래미안':2 , '아이파크':2, '자이':2,\n",
    "   #2군 하이엔드\n",
    "   '위브더제니스':2,'호반써밋':2,\n",
    "    #2군\n",
    "    '위브': 1,'우미린':1,'한화포레나':1, '서희스타힐스':1,'더플래티넘':1, '한라비발디':1,'호반베르디움':1, '데시앙':1, '센트레빌':1,'SKVIEW':1,''\n",
    "    '하늘채': 1,'스위첸':1\n",
    "}\n",
    "# 특정 문자열을 포함하는 경우에 따라 값을 반환하는 함수 정의\n",
    "def assign_brand_value(apartment_name):\n",
    "    for pattern, value in brand_patterns.items():\n",
    "        if pattern in apartment_name:\n",
    "            return value\n",
    "    return 0\n",
    "# '아파트브랜드이름'이라는 새로운 열 생성\n",
    "concat_select['아파트브랜드이름'] = concat_select['아파트명'].apply(assign_brand_value)\n",
    "concat_select = concat_select.drop(columns=['아파트명'])\n",
    "print(concat_select.shape, concat_select.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 금리 데이터 로드 및 처리\n",
    "# interest = pd.read_csv('/Users/minseok/Downloads/xgboost/data/한국기준금리07-23.csv')\n",
    "\n",
    "# interest['계약년월'] = interest['날짜'].astype(str).str[:6]\n",
    "\n",
    "# interest_grouped = interest.groupby('계약년월').agg({'기준금리': 'mean'}).reset_index()\n",
    "\n",
    "# concat_select['계약년월'] = concat_select['계약년'].astype(str) + concat_select['계약월'].astype(str)\n",
    "\n",
    "# concat_select = pd.merge(concat_select, interest_grouped, on='계약년월', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# print(concat_select.shape, concat_select.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_select = concat_select.drop(columns=['계약년월'])\n",
    "# print(concat_select.shape, concat_select.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 필요한 라이브러리 로드\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 금리 데이터 로드 및 처리\n",
    "# school_path = pd.read_csv('/Users/minseok/Downloads/xgboost/data/청주대학교_지방교육재정연구원_초중등학교위치_20240322.csv', encoding='EUC-KR')\n",
    "\n",
    "# # 학교별로 데이터를 나눔\n",
    "# h_school = school_path[school_path['학교급구분'] == '고등학교']\n",
    "# m_school = school_path[school_path['학교급구분'] == '중학교']\n",
    "# p_school = school_path[school_path['학교급구분'] == '초등학교']\n",
    "\n",
    "# # 각 학교의 좌표 추출\n",
    "# h_school_coords = np.array([h_school['경도'], h_school['위도']]).T\n",
    "# m_school_coords = np.array([m_school['경도'], m_school['위도']]).T\n",
    "# p_school_coords = np.array([p_school['경도'], p_school['위도']]).T\n",
    "\n",
    "# # 최단 거리 계산 함수\n",
    "# def calculate_shortest_distances(concat_coords, school_coords):\n",
    "#     shortest_distances = []\n",
    "#     for concat_coord in tqdm(concat_coords, total=len(concat_coords)):\n",
    "#         distances = haversine_np(concat_coord[0], concat_coord[1], school_coords[:, 0], school_coords[:, 1])\n",
    "#         shortest_distances.append(np.min(distances))\n",
    "#     return shortest_distances\n",
    "\n",
    "# # 고등학교, 중학교, 초등학교의 최단 거리 계산\n",
    "# concat_select['고등학교최단거리'] = calculate_shortest_distances(concat_coords, h_school_coords)\n",
    "# concat_select['중등학교최단거리'] = calculate_shortest_distances(concat_coords, m_school_coords)\n",
    "# concat_select['초등학교최단거리'] = calculate_shortest_distances(concat_coords, p_school_coords)\n",
    "\n",
    "# # 거리를 카테고리화하는 함수\n",
    "# def categorize_distance(distance):\n",
    "#     if 0 < distance < 0.5:\n",
    "#         return 3\n",
    "#     elif 0.5 < distance < 1.0:\n",
    "#         return 2\n",
    "#     elif 1.0 < distance < 1.5:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# # 각 학교별 최단 거리를 카테고리화\n",
    "# concat_select['초등학교최단거리'] = concat_select['초등학교최단거리'].apply(categorize_distance)\n",
    "# concat_select['중등학교최단거리'] = concat_select['중등학교최단거리'].apply(categorize_distance)\n",
    "# concat_select['고등학교최단거리'] = concat_select['고등학교최단거리'].apply(categorize_distance)\n",
    "\n",
    "# # 결과 확인\n",
    "# concat_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1099031, 29) (9272, 29)\n",
      "(1099031, 28) (9272, 28)\n"
     ]
    }
   ],
   "source": [
    "# 이제 다시 train과 test dataset을 분할해줍니다. 위에서 제작해 놓았던 is_test 칼럼을 이용합니다.\n",
    "dt_train = concat_select.query('is_test==0')\n",
    "dt_test = concat_select.query('is_test==1')\n",
    "\n",
    "# 결과 출력\n",
    "print(dt_train.shape, dt_test.shape)\n",
    "\n",
    "# 이제 is_test 칼럼은 drop해줍니다.\n",
    "dt_train.drop(['is_test'], axis = 1, inplace=True)\n",
    "dt_test.drop(['is_test'], axis = 1, inplace=True)\n",
    "print(dt_train.shape, dt_test.shape)\n",
    "\n",
    "# dt_test의 target은 일단 0으로 임의로 채워주도록 하겠습니다.\n",
    "dt_test['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 변수: ['전용면적', '계약일', '층', '건축년도', 'target', 'x', 'y', '계약년', '계약월', '거래일건물연식', '건축면적', '연면적', '대지면적', '건폐율', '용적율', '평균층수', '지하철최단거리', '버스최단거리', '아파트브랜드이름']\n",
      "범주형 변수: ['번지', '본번', '부번', '주소', '거래취소여부', '구', '동', '강남여부', '신축여부']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  6.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# 파생변수 제작으로 추가된 변수들이 존재하기에, 다시한번 연속형과 범주형 칼럼을 분리해주겠습니다.\n",
    "continuous_columns_v2 = []\n",
    "categorical_columns_v2 = []\n",
    "\n",
    "for column in dt_train.columns:\n",
    "    if pd.api.types.is_numeric_dtype(dt_train[column]):\n",
    "        continuous_columns_v2.append(column)\n",
    "    else:\n",
    "        categorical_columns_v2.append(column)\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns_v2)\n",
    "print(\"범주형 변수:\", categorical_columns_v2)\n",
    "\n",
    "# 아래에서 범주형 변수들을 대상으로 레이블인코딩을 진행해 주겠습니다.\n",
    "\n",
    "# 각 변수에 대한 LabelEncoder를 저장할 딕셔너리\n",
    "label_encoders = {}\n",
    "\n",
    "# Implement Label Encoding\n",
    "for col in tqdm( categorical_columns_v2 ):\n",
    "    lbl = LabelEncoder()\n",
    "\n",
    "    # Label-Encoding을 fit\n",
    "    lbl.fit( dt_train[col].astype(str) )\n",
    "    dt_train[col] = lbl.transform(dt_train[col].astype(str))\n",
    "    label_encoders[col] = lbl           # 나중에 후처리를 위해 레이블인코더를 저장해주겠습니다.\n",
    "\n",
    "    # Test 데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가해줍니다.\n",
    "    for label in np.unique(dt_test[col]):\n",
    "      if label not in lbl.classes_: # unseen label 데이터인 경우\n",
    "        lbl.classes_ = np.append(lbl.classes_, label) # 미처리 시 ValueError발생하니 주의하세요!\n",
    "\n",
    "    dt_test[col] = lbl.transform(dt_test[col].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train = pd.get_dummies(dt_train, columns=['구'])\n",
    "dt_test = pd.get_dummies(dt_test, columns=['구'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete = ['번지', '부번', '본번']\n",
    "\n",
    "# # 열 삭제\n",
    "dt_train = dt_train.drop(columns=columns_to_delete)\n",
    "dt_test = dt_test.drop(columns=columns_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert dt_train.shape[1] == dt_test.shape[1]          # train/test dataset의 shape이 같은지 확인해주겠습니다.\n",
    "\n",
    "# # Target과 독립변수들을 분리해줍니다.\n",
    "# y_train = dt_train['target']\n",
    "# X_train = dt_train.drop(['target'], axis=1)\n",
    "\n",
    "# # Hold out split을 사용해 학습 데이터와 검증 데이터를 8:2 비율로 나누겠습니다.\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # '계약년' 컬럼의 2023 미만과 2023년 이상 비율을 출력하는 코드\n",
    "# def print_year_split_ratios(X_train, X_val, y_train, y_val):\n",
    "#     # 계약년이 2023 미만인 비율과 2023 이상인 비율을 계산하여 출력\n",
    "#     train_2023_below = (X_train['계약년'] < 2023).mean() * 100\n",
    "#     train_2023_above = (X_train['계약년'] >= 2023).mean() * 100\n",
    "#     val_2023_below = (X_val['계약년'] < 2023).mean() * 100\n",
    "#     val_2023_above = (X_val['계약년'] >= 2023).mean() * 100\n",
    "    \n",
    "#     print(f\"Training Set - 계약년 2023 미만 비율: {train_2023_below:.2f}%\")\n",
    "#     print(f\"Training Set - 계약년 2023 이상 비율: {train_2023_above:.2f}%\")\n",
    "#     print(f\"Validation Set - 계약년 2023 미만 비율: {val_2023_below:.2f}%\")\n",
    "#     print(f\"Validation Set - 계약년 2023 이상 비율: {val_2023_above:.2f}%\")\n",
    "\n",
    "# # 함수 호출 (데이터 준비 후 실행)\n",
    "# print_year_split_ratios(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  XGBoost 사용\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# model = xgb.XGBRegressor(objective='reg:squarederror', \n",
    "#                          n_estimators=2000,  \n",
    "#                          learning_rate=0.1,  \n",
    "#                          max_depth=10,\n",
    "#                          random_state=1)\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# pred_2 = model.predict(X_val)\n",
    "\n",
    "# # 회귀 관련 metric을 통해 train/valid의 모델 적합 결과를 관찰합니다.\n",
    "# print(f'RMSE test: {np.sqrt(metrics.mean_squared_error(y_val, pred_2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# import joblib\n",
    "# import os\n",
    "\n",
    "# # RMSE 계산 함수\n",
    "# def RMSE(y, y_pred):\n",
    "#     return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# # 하이퍼파라미터 그리드 설정\n",
    "# param_grid = {\n",
    "#     'n_estimators': [9000, 10000, 11000, 12000],  # 더 높은 n_estimators 설정\n",
    "#     'learning_rate': [0.05, 0.075, 0.1],  # 학습률을 더 세밀하게 설정\n",
    "#     'max_depth': [5, 6, 7],  # max_depth 범위를 확장하여 조정\n",
    "#     'min_child_weight': [1, 3],  # 현재 최적값 유지\n",
    "#     'subsample': [0.8],  # 그대로 유지\n",
    "#     'colsample_bytree': [0.8],  # 그대로 유지\n",
    "#     'gamma': [0],  # 그대로 유지\n",
    "#     'lambda': [1],  # 그대로 유지\n",
    "#     'alpha': [0]  # 그대로 유지\n",
    "# }\n",
    "\n",
    "# # 체크포인트 경로 설정\n",
    "# checkpoint_path = \"grid_search_checkpoint.pkl\"\n",
    "\n",
    "# # 체크포인트에서 학습 재개\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     start_idx, best_params, best_rmse = joblib.load(checkpoint_path)\n",
    "#     print(f\"Checkpoint 로드 완료, {start_idx}번째 파라미터부터 재개.\")\n",
    "# else:\n",
    "#     start_idx = 0\n",
    "#     best_params = None\n",
    "#     best_rmse = float('inf')\n",
    "#     print(\"새로운 학습 시작.\")\n",
    "\n",
    "# # XGBRegressor 모델 정의\n",
    "# xgb_model = xgb.XGBRegressor(\n",
    "#     objective='reg:squarederror',\n",
    "#     random_state=2024\n",
    "# )\n",
    "\n",
    "# # 파라미터 그리드 순회\n",
    "# for idx, params in enumerate(ParameterGrid(param_grid)):\n",
    "#     if idx < start_idx:\n",
    "#         continue  # 이전에 완료된 파라미터 조합은 건너뜀\n",
    "    \n",
    "#     # 모델 학습\n",
    "#     print(f\"{idx + 1}번째 파라미터 조합 학습 중: {params}\")\n",
    "#     xgb_model.set_params(**params)\n",
    "#     xgb_model.fit(X_train, y_train)\n",
    "\n",
    "#     # 검증 세트에서 예측\n",
    "#     pred_val = xgb_model.predict(X_val)\n",
    "#     val_rmse = RMSE(y_val, pred_val)\n",
    "#     print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "#     # 최적 파라미터 갱신\n",
    "#     if val_rmse < best_rmse:\n",
    "#         best_rmse = val_rmse\n",
    "#         best_params = params\n",
    "\n",
    "#     # 진행 상황을 체크포인트에 저장\n",
    "#     joblib.dump((idx + 1, best_params, best_rmse), checkpoint_path)\n",
    "#     print(f\"Checkpoint 저장됨: {checkpoint_path}\")\n",
    "\n",
    "# # 최적 파라미터 및 성능 출력\n",
    "# print(f\"최적 파라미터: {best_params}, Best RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# # 최적 모델로 학습 후 모델 저장\n",
    "# best_xgb_model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=2024)\n",
    "# best_xgb_model.fit(X_train, y_train)\n",
    "# best_xgb_model.save_model(f\"xgb_best_model_rmse_{best_rmse:.4f}.model\")\n",
    "\n",
    "# # 체크포인트 삭제\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     os.remove(checkpoint_path)\n",
    "#     print(\"Checkpoint 파일 삭제 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train 상위 10% 값의 개수: 88049개\n",
      "y_val 상위 10% 값의 개수: 22242개\n",
      "y_train 상위 10% 값 (내림차순):\n",
      "[1450000. 1350000. 1300000. ...  109000.  109000.  109000.]\n",
      "y_val 상위 10% 값 (내림차순):\n",
      "[1100000. 1080000.  970000. ...  108000.  108000.  108000.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_train, y_val에서 상위 10%에 해당하는 임계값을 계산\n",
    "top_10_percent_train_threshold = np.percentile(y_train, 90)  # 상위 10%의 임계값 계산\n",
    "top_10_percent_val_threshold = np.percentile(y_val, 90)      # 검증 데이터 상위 10% 임계값 계산\n",
    "\n",
    "# y_train에서 상위 10%에 해당하는 값 필터링 후 정렬 (내림차순)\n",
    "top_10_percent_train = np.sort(y_train[y_train >= top_10_percent_train_threshold])[::-1]\n",
    "# y_val에서 상위 10%에 해당하는 값 필터링 후 정렬 (내림차순)\n",
    "top_10_percent_val = np.sort(y_val[y_val >= top_10_percent_val_threshold])[::-1]\n",
    "\n",
    "# 상위 10%에 해당하는 값의 개수 출력\n",
    "print(f\"y_train 상위 10% 값의 개수: {len(top_10_percent_train)}개\")\n",
    "print(f\"y_val 상위 10% 값의 개수: {len(top_10_percent_val)}개\")\n",
    "\n",
    "# 상위 10%에 해당하는 값 출력 (내림차순 정렬)\n",
    "print(f\"y_train 상위 10% 값 (내림차순):\\n{top_10_percent_train}\")\n",
    "print(f\"y_val 상위 10% 값 (내림차순):\\n{top_10_percent_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 학습 시작.\n",
      "최적 파라미터로 XGBoost 학습 시작: {'n_estimators': 12000, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'lambda': 1, 'alpha': 0}\n",
      "Validation RMSE: 5421.1373\n",
      "Checkpoint 파일 삭제 완료.\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import joblib\n",
    "# import os\n",
    "\n",
    "# # RMSE 계산 함수\n",
    "# def RMSE(y, y_pred):\n",
    "#     return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# # 가중치 부여 함수\n",
    "# def calculate_sample_weights(target, threshold=107000, high_weight=3, low_weight=1):\n",
    "#     \"\"\"\n",
    "#     집값이 threshold 이상이면 high_weight, 이하이면 low_weight 가중치를 부여\n",
    "#     \"\"\"\n",
    "#     return np.where(target >= threshold, high_weight, low_weight)\n",
    "\n",
    "# # 최적 파라미터 설정\n",
    "# best_params = {\n",
    "#     'n_estimators': 12000,  \n",
    "#     'learning_rate': 0.05,  \n",
    "#     'max_depth': 7,  \n",
    "#     'min_child_weight': 1,  \n",
    "#     'subsample': 0.8,  \n",
    "#     'colsample_bytree': 0.8,  \n",
    "#     'gamma': 0,  \n",
    "#     'lambda': 1,  \n",
    "#     'alpha': 0  \n",
    "# }\n",
    "\n",
    "# # 체크포인트 경로 설정\n",
    "# checkpoint_path = \"best_model_checkpoint.pkl\"\n",
    "\n",
    "# # 체크포인트에서 학습 재개\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     print(\"Checkpoint 로드 완료.\")\n",
    "#     best_xgb_model = joblib.load(checkpoint_path)\n",
    "# else:\n",
    "#     print(\"새로운 학습 시작.\")\n",
    "\n",
    "# # XGBRegressor 모델 정의\n",
    "# best_xgb_model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=2024)\n",
    "\n",
    "# # 가중치 계산 (y_train에 대해 가중치를 부여)\n",
    "# sample_weights = calculate_sample_weights(y_train, threshold=1000000, high_weight=3, low_weight=1)\n",
    "\n",
    "# # 최적 모델 학습\n",
    "# print(f\"최적 파라미터로 XGBoost 학습 시작: {best_params}\")\n",
    "# best_xgb_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# # 검증 세트에서 예측\n",
    "# pred_val = best_xgb_model.predict(X_val)\n",
    "# val_rmse = RMSE(y_val, pred_val)\n",
    "# print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "# # 최종 모델 저장\n",
    "# best_xgb_model.save_model(f\"xgb_best_model_rmse_{val_rmse:.4f}.model\")\n",
    "# joblib.dump(best_xgb_model, checkpoint_path)  # 학습된 모델을 저장\n",
    "\n",
    "# # 체크포인트 삭제 (선택사항)\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     os.remove(checkpoint_path)\n",
    "#     print(\"Checkpoint 파일 삭제 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import joblib\n",
    "# import os\n",
    "\n",
    "# # RMSE 계산 함수\n",
    "# def RMSE(y, y_pred):\n",
    "#     return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# # 최적 파라미터 설정\n",
    "# best_params = {\n",
    "#     'n_estimators': 12000,  \n",
    "#     'learning_rate': 0.05,  \n",
    "#     'max_depth': 7,  \n",
    "#     'min_child_weight': 3,  \n",
    "#     'subsample': 0.8,  \n",
    "#     'colsample_bytree': 0.8,  \n",
    "#     'gamma': 0,  \n",
    "#     'lambda': 1,  \n",
    "#     'alpha': 0  \n",
    "# }\n",
    "\n",
    "# # RMSE 결과 출력\n",
    "# print(\"RMSE test: 5435.876848555208\")\n",
    "# print(f\"최적 파라미터: {best_params}\")\n",
    "# # 체크포인트 경로 설정\n",
    "# checkpoint_path = \"best_model_checkpoint.pkl\"\n",
    "\n",
    "# # 체크포인트에서 학습 재개\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     print(\"Checkpoint 로드 완료.\")\n",
    "#     best_xgb_model = joblib.load(checkpoint_path)\n",
    "# else:\n",
    "#     print(\"새로운 학습 시작.\")\n",
    "\n",
    "# # 로그 변환된 y_train과 y_val 사용\n",
    "# y_train_log = np.log(y_train)  # 타겟 변수를 로그 변환\n",
    "# y_val_log = np.log(y_val)  # 검증용 타겟 변수도 로그 변환\n",
    "\n",
    "# # XGBRegressor 모델 정의\n",
    "# best_xgb_model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=2024)\n",
    "\n",
    "# # 최적 모델 학습 (로그 변환된 타겟으로 학습)\n",
    "# print(f\"최적 파라미터로 XGBoost 학습 시작: {best_params}\")\n",
    "# best_xgb_model.fit(X_train, y_train_log)\n",
    "\n",
    "# # 검증 세트에서 예측 (로그 변환된 값으로 예측)\n",
    "# pred_val_log = best_xgb_model.predict(X_val)\n",
    "\n",
    "# # 로그 변환된 값과 비교하여 RMSE 계산\n",
    "# val_rmse_log = RMSE(y_val_log, pred_val_log)  # 로그 변환된 값에서 RMSE 계산\n",
    "# print(f\"Validation RMSE (로그 변환된 값): {val_rmse_log:.4f}\")\n",
    "\n",
    "# # 원래 값으로 변환한 후, 실제 RMSE 확인\n",
    "# pred_val = np.exp(pred_val_log)  # 예측 값 지수 변환\n",
    "# val_rmse = RMSE(y_val, pred_val)  # 원래 값과 비교하여 RMSE 계산\n",
    "# print(f\"Validation RMSE (지수 변환 후 실제 값): {val_rmse:.4f}\")\n",
    "\n",
    "# # 최종 모델 저장\n",
    "# best_xgb_model.save_model(f\"xgb_best_model_rmse_{val_rmse:.4f}.model\")\n",
    "# joblib.dump(best_xgb_model, checkpoint_path)  # 학습된 모델을 저장\n",
    "\n",
    "# # 체크포인트 삭제 (선택사항)\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     os.remove(checkpoint_path)\n",
    "#     print(\"Checkpoint 파일 삭제 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 학습 시작\n",
      "Fold 1 RMSE: 5394.4457\n",
      "Fold 2: 학습 시작\n",
      "Fold 2 RMSE: 5635.8242\n",
      "Fold 3: 학습 시작\n",
      "Fold 3 RMSE: 5649.0600\n",
      "Fold 4: 학습 시작\n",
      "Fold 4 RMSE: 5532.8477\n",
      "Fold 5: 학습 시작\n",
      "Fold 5 RMSE: 5788.1907\n",
      "Average RMSE across 5 folds: 5600.0737\n",
      "최적 파라미터로 XGBoost 학습 시작: {'n_estimators': 12000, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'lambda': 1, 'alpha': 0}\n",
      "Checkpoint 파일 삭제 완료.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# RMSE 계산 함수\n",
    "def RMSE(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# 로그 변환된 RMSE를 계산하기 위한 함수\n",
    "def calculate_rmse_with_kfold(X_train, y_train, best_params, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=2024)\n",
    "    fold_rmse = []\n",
    "    \n",
    "    # K-fold 교차 검증 시작\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # 로그 변환\n",
    "        y_train_log_fold = np.log(y_train_fold)\n",
    "        y_val_log_fold = np.log(y_val_fold)\n",
    "        \n",
    "        # XGBRegressor 모델 정의\n",
    "        xgb_model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=2024)\n",
    "        \n",
    "        # 모델 학습\n",
    "        print(f\"Fold {fold}: 학습 시작\")\n",
    "        xgb_model.fit(X_train_fold, y_train_log_fold)\n",
    "        \n",
    "        # 검증 세트에서 예측\n",
    "        pred_val_log = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # 원래 값으로 변환한 후, 실제 RMSE 확인\n",
    "        pred_val = np.exp(pred_val_log)\n",
    "        val_rmse = RMSE(y_val_fold, pred_val)\n",
    "        \n",
    "        # RMSE 결과 저장\n",
    "        fold_rmse.append(val_rmse)\n",
    "        print(f\"Fold {fold} RMSE: {val_rmse:.4f}\")\n",
    "    \n",
    "    # 모든 fold에 대한 평균 RMSE 출력\n",
    "    average_rmse = np.mean(fold_rmse)\n",
    "    print(f\"Average RMSE across {n_splits} folds: {average_rmse:.4f}\")\n",
    "    \n",
    "    return average_rmse\n",
    "\n",
    "# 최적 파라미터 설정\n",
    "best_params = {\n",
    "    'n_estimators': 12000,  \n",
    "    'learning_rate': 0.05,  \n",
    "    'max_depth': 7,  \n",
    "    'min_child_weight': 3,  \n",
    "    'subsample': 0.8,  \n",
    "    'colsample_bytree': 0.8,  \n",
    "    'gamma': 0,  \n",
    "    'lambda': 1,  \n",
    "    'alpha': 0  \n",
    "}\n",
    "\n",
    "# 데이터 로드 및 Target과 독립변수 분리\n",
    "y_train = dt_train['target']\n",
    "X_train = dt_train.drop(['target'], axis=1)\n",
    "\n",
    "# K-fold 적용하여 RMSE 계산\n",
    "average_rmse = calculate_rmse_with_kfold(X_train, y_train, best_params, n_splits=5)\n",
    "\n",
    "# 최종 학습 및 모델 저장\n",
    "checkpoint_path = \"best_model_checkpoint.pkl\"\n",
    "\n",
    "# 로그 변환된 y_train 사용\n",
    "y_train_log = np.log(y_train)\n",
    "\n",
    "# XGBRegressor 모델 정의 (최종 모델 학습)\n",
    "best_xgb_model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=2024)\n",
    "\n",
    "# 최적 모델 학습 (로그 변환된 타겟으로 학습)\n",
    "print(f\"최적 파라미터로 XGBoost 학습 시작: {best_params}\")\n",
    "best_xgb_model.fit(X_train, y_train_log)\n",
    "\n",
    "# 최종 모델 저장\n",
    "val_rmse = average_rmse  # K-fold의 평균 RMSE로 저장 파일명에 사용\n",
    "best_xgb_model.save_model(f\"xgb_best_model_rmse_{val_rmse:.4f}.model\")\n",
    "joblib.dump(best_xgb_model, checkpoint_path)  # 학습된 모델을 저장\n",
    "\n",
    "# 체크포인트 삭제 (선택사항)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "    print(\"Checkpoint 파일 삭제 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# # 예측 수행\n",
    "# pred_2 = best_xgb_model.predict(X_val)\n",
    "\n",
    "# # RMSE 계산\n",
    "# rmse_result = np.sqrt(metrics.mean_squared_error(y_val, pred_2))\n",
    "\n",
    "# # 결과 출력 및 파일 저장\n",
    "# print(f'RMSE test: {rmse_result}')\n",
    "\n",
    "# # 결과를 파일에 저장\n",
    "# with open(\"rmse_result.txt\", \"w\") as f:\n",
    "#     f.write(f'RMSE test: {rmse_result}\\n')\n",
    "#     f.write(f\"최적 파라미터: {best_params}, Best RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE test: 3572.750454907125\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# 예측 수행\n",
    "pred_2 = best_xgb_model.predict(X_train)\n",
    "pred_val = np.exp(pred_2)  # 예측 값 지수 변환\n",
    "rmse_result = RMSE(y_train, pred_val)  # 원래 값과 비교하여 RMSE 계산\n",
    "\n",
    "# # RMSE 계산\n",
    "# rmse_result = np.sqrt(metrics.mean_squared_error(y_val, pred_2))\n",
    "\n",
    "# 결과 출력 및 파일 저장\n",
    "print(f'RMSE test: {rmse_result}')\n",
    "\n",
    "# 결과를 파일에 저장\n",
    "with open(\"rmse_result.txt\", \"w\") as f:\n",
    "    f.write(f'RMSE test: {rmse_result}\\n')\n",
    "    f.write(f\"최적 파라미터: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 저장합니다. Pickle 라이브러리를 이용하겠습니다.\n",
    "with open('saved_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델을 불러옵니다.\n",
    "with open('saved_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = dt_test.drop(['target'], axis=1)\n",
    "\n",
    "# real_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dt_test.drop(['target'], axis=1)\n",
    "\n",
    "real_test_pred = model.predict(X_test)\n",
    "real_pred_val = np.exp(real_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 앞서 예측한 예측값들을 저장합니다.\n",
    "# preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "# preds_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(real_pred_val.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
